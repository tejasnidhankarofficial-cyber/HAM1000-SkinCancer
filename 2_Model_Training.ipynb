{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe1c167-4328-4fef-87f5-2b41c034e9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training device set to: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split # Needed if you didn't run the split yet\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = '/Users/tejii.03/Documents/Projects/Skin Cancer Detection/Skin-Cancer-Detection/data'\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, '/Users/tejii.03/Documents/Projects/Skin Cancer Detection/Skin-Cancer-Detection/data/images')\n",
    "TRAIN_METADATA_PATH = os.path.join(DATA_DIR, 'train_metadata.csv')\n",
    "VAL_METADATA_PATH = os.path.join(DATA_DIR, 'val_metadata.csv')\n",
    "LABEL_MAP = {'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, 'akiec': 4, 'vasc': 5, 'df': 6}\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# --- 1. M4 GPU (MPS) Check ---\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Training device set to: {DEVICE}\")\n",
    "\n",
    "# --- 2. PyTorch Dataset Class (Reuse the one we discussed) ---\n",
    "class HAM10000Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path, image_folder, transform=None):\n",
    "        self.data_frame = pd.read_csv(metadata_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_paths = {row['image_id']: os.path.join(image_folder, row['image_id'] + '.jpg')\n",
    "                            for index, row in self.data_frame.iterrows()}\n",
    "        self.data_frame['label'] = self.data_frame['dx'].map(LABEL_MAP)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        img_id = row['image_id']\n",
    "        label = row['label']\n",
    "        img_path = self.image_paths.get(img_id)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            return self.__getitem__((idx + 1) % self.__len__()) # Skip corrupt images\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- 3. Data Transforms ---\n",
    "# Ensure you have your data augmentation for training!\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Load DataLoaders ---\n",
    "# Assuming you have run the split and the CSVs exist:\n",
    "train_dataset = HAM10000Dataset(TRAIN_METADATA_PATH, IMAGE_FOLDER, train_transform)\n",
    "val_dataset = HAM10000Dataset(VAL_METADATA_PATH, IMAGE_FOLDER, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9417f3b7-9d00-4073-a31b-aaf2c3221874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "def setup_model(num_classes, device):\n",
    "    # 1. Load a pre-trained ResNet50 model\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # 2. Freeze all layers (optional but good for initial training)\n",
    "    # This prevents the pre-trained weights from changing much\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Replace the final classification layer\n",
    "    # ResNet50's classifier is called 'fc' (fully connected)\n",
    "    num_ftrs = model.fc.in_features # Get the number of features from the layer before 'fc'\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # 4. Move the model to the M4 GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = setup_model(NUM_CLASSES, DEVICE)\n",
    "print(\"Model loaded and moved to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0093458-53e7-446a-9727-02b9c1fa8e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Class Weights (on GPU): tensor([ 0.2134,  1.2860,  1.3021,  2.7848,  4.3686, 10.0401, 12.4410],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get the list of all labels from the training set\n",
    "train_labels = train_dataset.data_frame['label'].values\n",
    "\n",
    "# 2. Compute the balanced weights using scikit-learn\n",
    "# 'balanced' mode automatically adjusts weights inversely proportional to class frequencies\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# 3. Convert weights to a PyTorch tensor and move to GPU\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "print(\"Calculated Class Weights (on GPU):\", weights_tensor)\n",
    "\n",
    "# --- Define Loss and Optimizer ---\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor) # <-- Using the weighted loss!\n",
    "\n",
    "# Only optimize the parameters of the newly added final layer (model.fc)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# A scheduler to gradually decrease the learning rate is also a good idea\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65afd30f-e03d-477b-8b79-f4193822d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Move inputs and labels to the M4 GPU (MPS)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1) # Get the predicted class\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "\n",
    "    scheduler.step() # Update the learning rate\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # FIX APPLIED: Use .float() instead of .double() for MPS compatibility\n",
    "    epoch_acc = correct_predictions.float() / len(train_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a130a2f8-5515-4975-9f54-dfb07334d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Tells PyTorch not to calculate gradients (saves memory/time)\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for inputs, labels in val_loader:\n",
    "        # Move inputs and labels to the M4 GPU (MPS)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    \n",
    "    # FIX APPLIED: Use .float() instead of .double() for MPS compatibility\n",
    "    epoch_acc = correct_predictions.float() / len(val_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee05bc3-7e16-4bae-bea8-526beaa4d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n",
      "Epoch 1/50 - Train Loss: 0.9445, Train Acc: 0.6564 | Val Loss: 1.0091, Val Acc: 0.6370\n",
      "Epoch 2/50 - Train Loss: 0.9166, Train Acc: 0.6563 | Val Loss: 1.0199, Val Acc: 0.6535\n",
      "Epoch 3/50 - Train Loss: 0.9220, Train Acc: 0.6631 | Val Loss: 1.0046, Val Acc: 0.6445\n",
      "Epoch 4/50 - Train Loss: 0.9407, Train Acc: 0.6616 | Val Loss: 1.0071, Val Acc: 0.6440\n",
      "Epoch 5/50 - Train Loss: 0.9044, Train Acc: 0.6639 | Val Loss: 0.9948, Val Acc: 0.6575\n",
      "Epoch 6/50 - Train Loss: 0.9116, Train Acc: 0.6659 | Val Loss: 1.0006, Val Acc: 0.6420\n",
      "Epoch 7/50 - Train Loss: 0.9015, Train Acc: 0.6625 | Val Loss: 0.9915, Val Acc: 0.6535\n",
      "Epoch 8/50 - Train Loss: 0.9038, Train Acc: 0.6620 | Val Loss: 0.9982, Val Acc: 0.6545\n",
      "Epoch 9/50 - Train Loss: 0.9222, Train Acc: 0.6633 | Val Loss: 1.0035, Val Acc: 0.6455\n",
      "Epoch 10/50 - Train Loss: 0.9201, Train Acc: 0.6667 | Val Loss: 0.9992, Val Acc: 0.6560\n",
      "Epoch 11/50 - Train Loss: 0.8955, Train Acc: 0.6651 | Val Loss: 0.9993, Val Acc: 0.6555\n",
      "Epoch 12/50 - Train Loss: 0.9096, Train Acc: 0.6664 | Val Loss: 1.0055, Val Acc: 0.6550\n",
      "Epoch 13/50 - Train Loss: 0.9055, Train Acc: 0.6650 | Val Loss: 0.9949, Val Acc: 0.6650\n",
      "Epoch 14/50 - Train Loss: 0.8824, Train Acc: 0.6724 | Val Loss: 1.0060, Val Acc: 0.6410\n",
      "Epoch 15/50 - Train Loss: 0.9037, Train Acc: 0.6711 | Val Loss: 1.0153, Val Acc: 0.6505\n",
      "Epoch 16/50 - Train Loss: 0.8906, Train Acc: 0.6669 | Val Loss: 0.9912, Val Acc: 0.6600\n",
      "Epoch 17/50 - Train Loss: 0.8928, Train Acc: 0.6679 | Val Loss: 1.0015, Val Acc: 0.6450\n",
      "Epoch 18/50 - Train Loss: 0.9045, Train Acc: 0.6735 | Val Loss: 0.9980, Val Acc: 0.6385\n",
      "Epoch 19/50 - Train Loss: 0.9171, Train Acc: 0.6686 | Val Loss: 0.9922, Val Acc: 0.6500\n",
      "Epoch 20/50 - Train Loss: 0.8888, Train Acc: 0.6682 | Val Loss: 0.9912, Val Acc: 0.6555\n",
      "Epoch 21/50 - Train Loss: 0.9112, Train Acc: 0.6676 | Val Loss: 1.0076, Val Acc: 0.6445\n",
      "Epoch 22/50 - Train Loss: 0.9019, Train Acc: 0.6726 | Val Loss: 0.9927, Val Acc: 0.6525\n",
      "Epoch 23/50 - Train Loss: 0.9162, Train Acc: 0.6660 | Val Loss: 0.9980, Val Acc: 0.6590\n",
      "Epoch 24/50 - Train Loss: 0.8910, Train Acc: 0.6639 | Val Loss: 0.9980, Val Acc: 0.6505\n",
      "Epoch 25/50 - Train Loss: 0.9069, Train Acc: 0.6661 | Val Loss: 0.9950, Val Acc: 0.6395\n",
      "Epoch 26/50 - Train Loss: 0.8966, Train Acc: 0.6681 | Val Loss: 0.9970, Val Acc: 0.6540\n",
      "Epoch 27/50 - Train Loss: 0.9210, Train Acc: 0.6633 | Val Loss: 0.9961, Val Acc: 0.6490\n",
      "Epoch 28/50 - Train Loss: 0.9003, Train Acc: 0.6656 | Val Loss: 0.9983, Val Acc: 0.6635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     train_loss, train_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     12\u001b[39m     val_loss, val_acc = validate_model(model, val_loader, criterion, DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, scheduler, device)\u001b[39m\n\u001b[32m      3\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m      4\u001b[39m correct_predictions = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move inputs and labels to the M4 GPU (MPS)\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mHAM10000Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__getitem__\u001b[39m((idx + \u001b[32m1\u001b[39m) % \u001b[38;5;28mself\u001b[39m.\u001b[34m__len__\u001b[39m()) \u001b[38;5;66;03m# Skip corrupt images\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, torch.tensor(label, dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torchvision/transforms/transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:253\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myskincancer/lib/python3.11/site-packages/PIL/Image.py:2304\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2294\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2295\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2296\u001b[39m         )\n\u001b[32m   2297\u001b[39m         box = (\n\u001b[32m   2298\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2299\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2300\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2301\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2302\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Main Training Loop ---\n",
    "NUM_EPOCHS = 50 # Start with a small number to test the setup\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_model(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'skin_cancer_resnet50_final.pth')\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09c477-ecb5-492f-85f2-05da94dc12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
